\section{Methods} \label{sec:methods}

\input{methods_distance.tex}
\input{methods_k-dist.tex}
\input{methods_clustering.tex}

\subsection{Naive, greedy clustering algorithm}

A very simple, naive, greedy clustering algorithm, \textsc{Naive-Clust}, has
been implemented and will be described briefly in this section.

Given a list of sequences and an initially empty list of centroids, the
algorithm works by iterating through the sequences and for every sequence, it
iterates through the list of centroids and makes a comparison until a match is
found or until the end of the list is reached. If a match is found, the
sequence is added to the cluster represented by the matching centroid, and
otherwise the sequences becomes a new centroid and is appended to the end of
the list of centroids.

This approach is not very useful in practice since it is very computationally
expensive in the worst case. The worst case being quadratic in running time
when all sequences become centroids. The running time of the algorithm is very
dependent on the ordering of the sequences since the centroids will be in the
same order as in the list of sequences (i.e. the list of centroids is a
subsequence of the list of sequences) and if the first number of sequences
happens to be good centroids, which will match a lot of sequences, the
algorithm will perform good since the search for a centroid will end very
early. However, if the first sequences happen to be very dissimilar from the
rest of the data, then these will become centroids and will be compared
against on every iteration even though they might potentially never match any
other sequence.

Thus, there is a motivation to try and create more structure in the collection
of centroids, improve the search for a centroid or improve the ordering of the
centroids.

With added structure or better ordering of the centroids, it should ideally not
be necessary to traverse the whole collection of centroids for every sequence,
since a match should occur early on if it exists. This motivates the
introduction of a maximum allowed number of unsuccessful comparisons before
ending the search early and creating a new centroid.


\subsection{\textsc{Simple-Clust} algorithm}

Another clustering algorithm, named \textsc{Simple-Clust}, was designed and
implemented, and will be described in this section. \textsc{Simple-Clust} tries
to address some of the problems with the \textsc{Naive-Clust} algorithm by
imposing more structure on the collection of centroids and by giving up after
some fixed number of comparisons of a given query sequence.

\textsc{Simple-Clust} works by iterating sequentially through the sequences to
be clustered; the $max\_rejects$ most frequent $k$-mers for the query sequence
are calculated and for each of these most frequent $k$-mers, a centroid which
has that $k$-mer as the most frequently occurring one (if one such centroid
exists) is compared with the query sequence to check if their similarity is
above the given threshold. The query sequence is assigned to the cluster for
the first centroid that matches the query sequence; if no such centroid is
found, out of the maximum possible $max\_rejects$ number of tries, the query
sequence becomes a centroid for a new cluster and is added to the collection of
centroids along with the information about the most frequently occurring
$k$-mer in the sequence. Pseudocode for \textsc{Simple-Clust} is attached in
appendix \ref{app:simple-clust}.

However, there is a problem with this algorithm: For larger $k$ the number of
different possible $k$-mers increases exponentially and for e.g. $k = 6$ and a
four letter alphabet, there are $4^6 = 4096$ different $k$-mers and therefore
the lookup in line \ref{alg:line:simple_clust_lookup} of the algorithm will
often be unsuccessful.


\subsection{\textsc{K-Clust} algorithm}
\label{sec:k-clust_algorithm}

This section describes another clustering algorithm that was developed and
implemented, named \textsc{K-Clust}, which looks at the sets of $k$-mers
occurring in the query and target sequences and decides whether to try
comparing or not based on the cardinality of the intersection between the two
sets. This way, the search for a likely match is not just determined by the
single most frequently occurring $k$-mer, as with \textsc{Simple-Clust}, but
rather all the occurring $k$-mers, regardless of their count. Additionally, the
centroids are stored in a doubly linked list and whenever a centroid matches a
sequence, that centroid is moved to the front of the list. This makes the
algorithm less sensitive to the ordering of the input sequences. The choice of
the centroids will still be made greedily though, in lack of well-performing
alternatives, but the algorithm should nonetheless be less prone to bad
performance caused by an unlucky ordering of the input sequences. Algorithm
\ref{alg:k-clust} shows pseudocode for the \textsc{K-Clust} algorithm.

The choice of whether to compare a query sequence with a centroid, referred to
as the \emph{intersection criterion}, is decided using the formula
\[
  \abs{K(s) \cap K(c)} \geq \abs{K(c)} \cdot id \;
\]
where $s$ is the query sequence, $c$ is the centroid sequence and $id$ is the
similarity threshold. $K(x)$ denotes the number of $k$-mers in a sequence $x$.
In the \texttt{klust} program this value is hard coded to $k=6$ for performance
reasons.

The idea of moving a matching centroid to the front of the list is, among other
things, inspired by the observation that the results from \texttt{USEARCH} and
\texttt{VSEARCH} often contain a large number of very small clusters and even
singleton clusters, i.e. clusters consisting of just a single sequence.

To further improve the search for a centroid and add more structure to the
collection of centroids, the concept of a \emph{link} is introduced. A link is
a pointer from one centroid to another centroid which passed the intersection
criterion at an earlier point in clustering process. For each sequence, this is
managed by maintaining a link to the most recent centroids that have been
compared to. If a sequence becomes a centroid, this link will be saved along
with the sequence. Not every centroid will have a link, since it depends on a
passed intersection criterion which did not give a match.

When searching for a centroid, the link is used only when the intersection
criterion is passed for some centroid, but the comparison with that centroid was
unsuccessful and if that centroid happens to have a link to another centroid.
Comparisons with linked centroids does not increment the $rejects$ variable.
The \textsc{Has-Link} function in the pseudocode performs the check for whether
a centroid has a link or not.

An illustration of the \textsc{K-Clust} algorithm is seen in Figure
\ref{fig:k-clust}.

\begin{figure}[h!]
  \centering
  \def\svgwidth{0.9\columnwidth}
  \import{graphics/}{K-Clust.pdf_tex}
  \caption{Illustration of \textsc{K-Clust}. The centroid labeled
    $3$ is pushed to the front of the doubly-linked list because it was the 
    latest match to a sequence. The centroids labeled $4$ and $5$ has a link to 
    the centroids $1$ and $0$ respectively because they have been close to 
    matching them.}
  \label{fig:k-clust}
\end{figure}


\begin{algorithm}[h!]
  \caption{\textsc{K-Clust}}
  \label{alg:k-clust}
  \begin{algorithmic}[1]
    \Require{$S$ is an array of [DR]NA sequences, $k \in \mathbb{Z}^+$,
             $id \in [0,1]$ and $max\_rejects \in \mathbb{Z}^+$}
    \Statex
    \Function{K-Clust}{$S, k, id, max\_rejects$}
      \State $centroids \gets [~]$ \Comment{initialize empty list of centroids}
      \ForAll{$s \in S$}
        \State $match \gets false$
        \State $rejects \gets 0$
        \State $close\_match \gets \mathtt{NULL}$
        \ForAll{$c \in centroids$}
          \If{$rejects$ \texttt{==} $max\_rejects$}
            \State $break$
          \EndIf
          \State
          \LineComment{$K(x)$: set of $k$-mers in $x$}
          \If{$|K(s) \cap K(c)| \geq |K(c)| \cdot id$}
            \If{\Call{\textsc{K-Dist}}{s, c, k} $ \geq id$}
              \State add $s$ to cluster represented by $c$
              \State move $c$ to the front of $centroids$
              \State $match \gets true$
              \State $break$
            \EndIf
            \State
            \If{\Call{Has-Link}{c} \texttt{AND}
                  \Call{\textsc{K-Dist}}{s, c.link, k} $ \geq id$}
              \State add $s$ to cluster represented by $c.link$
              \State move $c.link$ to the front of $centroids$
              \State $match \gets true$
              \State $break$
            \EndIf
            \State $rejects \gets rejects + 1$
            \State $close\_match \gets c$
          \EndIf
        \EndFor
        \State
        \If{$\mathtt{NOT}\ match$}  \Comment{add new centroid to list}
          \If{$close\_match$ \texttt{!= NULL}}
            \State $s.link \gets close\_match$
          \EndIf
          \State prepend $s$ to $centroids$
        \EndIf
      \EndFor
    \EndFunction
  \end{algorithmic}
\end{algorithm}


\subsubsection{Time complexity analysis}

The \textsc{K-Clust} algorithm iterates over the array of sequences $S$, that
is $|S|$ iterations, and for each of these iterations, the algorithm iterates
over the list of centroids. In the worst case, the $centroids$ list will grow
with one element in each iteration of the outer loop, because in the worst
case, every sequence will become a centroid of its own. Thus, the number of
iterations in the inner loop will be the sequence
\[
  1 + 2 + 3 + \ldots + |S| \,,
\]
that is, the $|S|$'th triangular number, which can be expresses as
\begin{equation}
  \frac{\abs{S}\left(\abs{S}+1\right)}{2} \,. \label{eq:triangular_number}
\end{equation}

The inner loop contains a constant amount of work and at most two calls to
\textsc{K-Dist}, which will also be treated as constant time in this context,
since the running time of the clustering algorithm will be expressed in the
number of sequences, rather than the lengths of the sequences. The $K(s) \cap
K(c)$ calculation of the intersection between the two sets of $k$-mers in
sequences $s$ and $c$, respectively, will also be treated as constant since it
also depends on the lengths of $s$ and $c$ and not on the number of sequences
to be clustered.

Thus, the \textsc{K-Clust} algorithm iterates over the constant amount of work
in the inner loop the number of times expressed in (\ref{eq:triangular_number})
and so the running time is quadratic in the number of sequences, i.e.:
\begin{equation}
  \mathcal{O}\left(\abs{S}^2\right)
\end{equation}

This is, however, just the worst case running time and on average the running
time will be much better since the number of centroids will be much lower than
the total number of sequences.

% TODO: (maybe) analyze space complexity
