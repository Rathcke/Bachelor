\subsection{Cluster analysis algorithms}

There are several approaches to clustering sequence data. This section
describes different clustering paradigms and it discusses the strengths,
weaknesses and potentials of some of these methods.

\subsubsection{Hierarchical clustering}

\emph{Hierarchical clustering} is typically divided into two types of
clustering algorithms, the agglomerative type that works in a bottom-up manner
and the divisive type that has a top-down approach~\cite{dong}. This section
describes the agglomerative version. In the agglomerative version all sequences
start as leaves in a tree, where the leaves are considered clusters. Each
cluster is then merged together with the closest cluster, thus building a tree,
level for level, until there is only one cluster left. This gives a hierarchy
of the clusters. Alternatively, when the clustering is based on a similarity
threshold, as in this project, the clusters are merged until no more clusters
can be merged. Agglomerative clustering is visualized in Figure
\ref{fig:hierarchical_clustering}.

\begin{figure}[h!]
  \centering
  \def\svgwidth{\columnwidth}
  \import{graphics/}{Hierarchical_Clustering.pdf_tex}
  \caption{Illustration of agglomerative hierarchical clustering. Clusters,
  with objects represented as letters, are merged with nearby clusters based
  on the distance between their positions in the alphabet until there is only
  one cluster.}
  \label{fig:hierarchical_clustering}
\end{figure}

This addresses the question of what is required for a cluster to be ``close''
to another. Besides measuring similarity between sequences with some similarity
metric, a definition for when clusters should be merged is needed. The
following describes different approaches for merging two clusters based on a
similarity threshold.

In the \textit{complete-linkage} approach, all sequences from one cluster must
have a similarity above the threshold to all sequences from the other cluster.
In the \textit{average-linkage}, the average similarity between all pairs of
sequences is required to be above the threshold. With the
\textit{single-linkage} approach, only two sequences, one from each cluster,
must have a similarity to each other above the similarity threshold.

The complete-linkage approach is computationally very heavy as each merge will
take $\mathcal{O}(mn)$ time, where $m$ and $n$ are the sizes of the clusters.
The average-linkage method is equally computationally hard. This is not viable
for a quick clustering algorithm.

The single-linkage approach is often used in sequence
clustering~\cite[pp.~62--63]{dong}. It has the advantage of behaving greedily
and stopping early if a link meets the similarity criterion instead of
calculating all links to find the minimal distance. Though the greedy behavior
can boost performance, it could result in a very rough clustering as two
sequences from the clusters might be very similar while other sequences could
have a very low similarity to each other.

An example of an agglomerative algorithm using the single-linkage approach is
SLINK~\cite{sibson} with complexity $\mathcal{O}\left(n^2\right)$. Another
example, using the complete-linkage approach, is CLINK~\cite{defays}, which is
based on SLINK and also has a complexity of $\mathcal{O}\left(n^2\right)$. As
the naive agglomerative method, both of these algorithms build a distance
matrix and then use row and column operations, but they are optimized to
running times of $\mathcal{O}\left(n^2\right)$ instead of
$\mathcal{O}\left(n^3\right)$, where $n$ is the number of sequences.

A running time of $\mathcal{O}\left(n^2\right)$ does not scale well enough for
problem sizes encountered in sequence clustering and it does not need to build
the entire hierarchy. However, with a greedy single-linkage approach and
without building the entire hierarchy, the complexity could be significantly
reduced. The distance matrix itself takes $\mathcal{O}\left(n^2\right)$ time
to create, which is already too much, so this indicates that a heuristic
strategy might be needed.


\subsubsection{Graph-based clustering}

The general framework of graph-based clustering consists of two steps. First
step is to generate a weighted graph from the sequences. The second step, the
clustering step, is to split the graph into subgraphs, which correspond to the
clusters~\cite[pp. 64-65]{dong}.

Consider a graph $G$ with a set of vertices $V$ and a set of edges $E$. In
graph based clustering, we consider $V$ as the set of sequences and $E$ as the
set of similarity scores. The first step is to calculate all edges $E_{ij}$,
the similarity score between $V_i$ and $V_j$, based on some similarity metric.
If the clustering is based on a similarity threshold, edges that are below the
similarity threshold can be ignored.

There are many variants to the clustering step. E. Hartuv and R. Shamir \\
\cite{hartuv} presents an algorithm that tries to minimize edges that
connect two subgraphs by moving vertices from one subgraph to another.

H. Kawaji \textit{et al.}~\cite{kawaji} presents an algorithm that tries to
maximize the density of edges in a subgraph. This is done by converting the
graph into a simpler graph by removing all edges that are under the similarity
threshold.

Since both of these require that all edges' weights have been computed, it is
already too slow for efficient sequence clustering. If the subgraphs could be
constructed while computing distances, the time complexity could be reduced.
Both hierarchical and graph based clustering indicates that a heuristic
clustering approach is needed to obtain the desired performance.


\subsubsection{Greedy algorithm like \texttt{UCLUST}}

The \texttt{UCLUST} algorithm is greedy and is designed so that all member
sequences have similarity greater than or equal to the similarity threshold,
to their centroid.  It works by processing one sequence at a time and
comparing these to the existing centroids. If a match is found, the sequence
is assigned to the cluster of the matching centroid, otherwise it becomes a
centroid of a new cluster. \texttt{UCLUST} attempts to give a clustering
result where each centroid has a similarity under the threshold to all other
centroids, but this is not guaranteed to hold. The reason is that the
algorithm only compares a sequence to a prespecified number of centroids given
by the $max\_rejects$ parameter. It uses the $k$-mer counts to locate centroid
candidates for a match, though how it does it is not very well documented.

The similarity calculations are performed with global sequence alignments as R.
C. Edgar~\cite{usearch_algorithm}  claims that using the word counts to
calculate identity is not sensitive enough. Greedy clustering is illustrated in
Figure \ref{fig:greedy_clustering}.

\begin{figure}[H]
  \def\svgwidth{\columnwidth}
  \import{graphics/}{Greedy_Clustering.pdf_tex}
  \caption{Illustration of greedy clustering. Sequences are processed in the
    order they are read. They either become a centroid for a new cluster or
    they are assigned to an existing cluster.}
  \label{fig:greedy_clustering}
\end{figure}

Greedy clustering can reduce the time complexity as non-centroid sequences are
only handled once. This can also improve space complexity. The time complexity
is dependent on the number of centroids which is typically much lower than the
number of sequences. An efficient search for centroids can reduce the expected
running time to near linear as opposed to the worst case
$\mathcal{O}\left(n^2\right)$ complexity, where $n$ is the number of sequences
(in the case where all sequences become centroids).
