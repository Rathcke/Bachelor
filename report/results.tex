\section{Results and evaluation} \label{sec:results}

\subsection{Overview of datasets used for testing}

\texttt{SILVA}, \texttt{RDP}. Lengths of sequences, dataset size, lots of
errors?, RNA/DNA etc.


\subsection{Testing the distance metric on altered sequences}

Two different sets of highly similar sequences was constructed from a real
world sequence from \texttt{SILVA} and this was used in the evaluation of the
distance metric.

One real sequence was chosen, 10 copies of this sequences were made and for
each of these copies, a few alterations were made. In the first set, a list of
random indices in the sequence were generated and in each of these indices in
the sequence, a substitution for a different, randomly chosen nucleotide was
made. This was done twice, for different degrees of alteration, and the number
of alterations was equal to respectively 0.1\% and 0.2\% of the length of the
sequence.

In the second set, the same number of alterations to individual nucleotides
were made, but in this case they were grouped into alterations of substrings of
length 5 (one such group being of length less than 5 if the number of
alterations were not a multiple of 5).

For illustration, figure \ref{fig:alterations} shows an original sequence in
the first line, the single edits alterated sequence in the second line and the
chunk alterated sequence in the third line.

\newcommand{\tc}[1]{\textcolor{red}{#1}}
\begin{figure}[H]
  \centering
  \texttt{AAAAAAAAAAAAAAAAAAAAAA} \\
  \texttt{AAAAA\tc{T}AA\tc{C}AAAA\tc{G}AAAA\tc{T}AAA} \\
  \texttt{AAA\tc{TC}AAAAAAAAAA\tc{GG}AAAAA}
  \caption{Two types of alterations to sequences: the original sequence in the
    first line, single edit alterations in the second line and chunks of edits
    in the third line.}
  \label{fig:alterations}
\end{figure}

The $k$-mer based distance metric, presented in section
\ref{sec:kmer_distance}, is expected to be more sensitive to a number of
individual changes than to the same number of changes made in chunks. This test
serves to confirm this expectation and to give an insight into the change in
$k$-mer distance when performing a controlled number of edits.

\begin{figure}[H]
  \centering
\begin{tabular}{c|c||c|c}
  \multicolumn{2}{c||}{single edits} & \multicolumn{2}{c}{chunk edits} \\
  \hline\hline
  0.1\%   &   0.2\%                 &   0.1\%   &   0.2\%             \\
  \hline
  0.944 & 0.889 & 0.993 & 0.984 \\
  0.947 & 0.887 & 0.993 & 0.984 \\
  0.944 & 0.893 & 0.993 & 0.98  \\
  0.941 & 0.902 & 0.99  & 0.987 \\
  0.944 & 0.894 & 0.993 & 0.984 \\
  0.947 & 0.894 & 0.997 & 0.978 \\
  0.941 & 0.895 & 0.99  & 0.978 \\
  0.945 & 0.891 & 0.99  & 0.987 \\
  0.945 & 0.889 & 0.99  & 0.984 \\
  0.946 & 0.897 & 0.99  & 0.984
\end{tabular}
\end{figure}


\subsection{Constructing a synthetic dataset}

A synthetic dataset was constructed for evaluating the distance metric on
different types of alterations to sequences and for evaluating the clustering
algorithm's ability to find the right clusters in a dataset with a clearly
correct, expected clustering output.

A set of sequences was chosen from \texttt{SILVA} by clustering with a
similarity threshold of $0.5$ and choosing the 20 first centroids from the
result. These centroids were confirmed to be dissimilar by calculating the
distance matrix for the sequences and checking that all the similarities were
below $0.5$.


\subsection{Evaluating distance metric}

To evaluate our implementation of the $k$-mer distance using windows and the
Jaccard index, the Levenshtein distance was used as reference. Since the
Levenshtein distance does not produce values between $0$ and $1$, the Jaccard
index was used here as well.

In figure \ref{fig:Levenshtein_vs_Kmer} is three scatter plots with the
Levenshtein distance on the $y$-axis and our version of the $k$-mer distance on
the $x$-axis. The three plots are with $k=4, k=6$ and $k=8$.

The scatter plots gives an indication about when and how sensitive our
implemented distance metric is. We want the distance to be as close as possible
to a linear function without too much variance, but not necessarily to the line
$y=x$. If it can be expressed linearly it means there is a correlation between
the two. If it can be expressed as $y=x$ (or close) it means that they measure
the same distance which is favourable.

When $k=4$ there is a linear relation when the identity $I>0.9$, but when
$I<0.9$ there is too much variance in the points from any linear function.

With $k=6$ there seems to be a linear relation when $I>0.8$. When $I<0.8$ there
is more variance though less than when $k=4$.

When $k=8$, it is very similar to $k=6$. It seems there is a linear relation
for $I>0.8$ again and when $I<0.8$ the variance is greater.

\begin{figure}
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[scale=0.34]{graphics/k4.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[scale=0.34]{graphics/k6.png}
  \end{subfigure}

  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[scale=0.34]{graphics/k8.png}
  \end{subfigure}
  \caption{Comparison of Levenshtein distance and our implementation of the
  $k$-mer distance using windows and Jaccard index.}
  \label{fig:Levenshtein_vs_Kmer}
\end{figure}


\subsection{Comparing with UCLUST on real life data}
% Testing USEARCH 32-bit on real data
% Testing clustering algorithm with d2 distance and comparing performance to
% USEARCH.
Running \texttt{USEARCH} on the file
\texttt{SILVA\_119\_SSURef\_tax\_silva.fasta} after it is sorted with
parameters \texttt{-clust\_smallmem} and \texttt{-id 0.95} produces the
following output

\begin{lstlisting}[style=output-style,
  caption=Output from \texttt{USEARCH} clustering.,
  label=fig:uclust_silva]
28:39 1.1Gb  100.0\% 117205 clusters, max size 83904, avg 13.5
      Seqs  1583830 (1.6M)
  Clusters  117205 (117.2k)
  Max size  83904 (83.9k)
  Avg size  13.5
  Min size  1
Singletons  67410 (67.4k), 4.3\% of seqs, 57.5\% of clusters
   Max mem  1.1Gb
      Time  28:41
Throughput  920.3 seqs/sec.
\end{lstlisting}

Running our implementation of the same file but unsorted and with
\texttt{id=0.9, k=6} og \texttt{max\_rejects=8} produces the following

\begin{lstlisting}[style=output-style]
Reading 1583830 sequences...
Finished reading:
Time: 24.8109 sec.
Seqs/sec: 63836
Clustering 1583830 sequences...
26.4602
# of clusters: 1364006
Finished clustering:
Time: 187.355 sec.
Throughput: 8453.63seqs/sec.
\end{lstlisting}

Even though the \texttt{id}'s does not represent the same threshold due to it
being two different distance metrics we do get a lot more clusters. Running
it with similar \texttt{id}'s produces almost the same amount of clusters, so
the problem lies in the way we choose our centroids we compare a query
sequence with.

Our implementation, however, is faster by a factor of $10$. This can be
attributed to our choice of distance metric.

%Possibly things to look into: confusion matrix, Rand index, normalized
%mutual information (article: Comparing Clusterings - An Overview).


\subsection{Evaluating clustering algorithm on real data}

This section describes one way of evaluating the
\textsc{Prioritized\_Intersect\_Clust} algorithm on real \texttt{RNA} data
(\texttt{SILVA}) using multidimensional scaling (MDS) and t-distributed
Stochastic Neighbor Embedding (t-SNE). This was done using \texttt{Python} with
\texttt{matplotlib} and \texttt{scikit-learn}.

One example of this
multidimesional scaling on the distance matrix for the first 500 sequences of
\texttt{SILVA} is shown in figure \ref{fig:mds_tsne}. The centroids found using
the clustering algorithm \textsc{Prioritized\_Intersect\_Clust}, with similarity
threshold 0.85, $max\_rejects$ 8 and $k$ 6, are pinpointed in the plot and
labeled with the numbers of the sequences in the order they were read.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{graphics/MDS_t-SNE_SILVA_500.png}
  \caption{Multidimensional scaling with annotations on centroids.}
  \label{fig:mds_tsne}
\end{figure}


\subsection{Results}
\begin{figure}[H]
  \centering
  \begin{tabular}{ c | c }
    Metric                                        & Comparisons/second      \\
    \hline \hline
    Dynamic programming (bottom up) Levenshtein   & $\sim$ 70               \\
    \hline
    d2-distance with window, $k=4$                & $\sim$ 73000            \\
    \hline
    d2-distance with window, $k=6$                & $\sim$ 63000            \\
    \hline
    d2-distance with window, $k=8$                & $\sim$ 24000            \\
  \end{tabular}
  \caption{Performance of different distance metrics.}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{tabular}{ p{12em} | c | c }
    Method  & Throughput/second   & \# of clusters \\
    \hline \hline
    \textsc{Simple\_Clust}, $k=4$,
    $max\_rejects=8$, $id=0.97$     & $\sim$ 11,400  & 444,654  \\
    \hline
    \textsc{Simple\_Clust}, $k=5$,
    $max\_rejects=8$, $id=0.97$     & $\sim$ 10,700  & 461,266  \\
    \hline
    \textsc{Simple\_Clust}, $k=6$,
    $max\_rejects=8$, $id=0.97$     & $\sim$ 9,575   & 470,516  \\
    \hline
    \textsc{Simple\_Clust}, $k=7$,
    $max\_rejects=8$, $id=0.97$     & $\sim$ 6,350   & 474,463  \\
    \hline
    \textsc{Simple\_Clust}, $k=8$,
    $max\_rejects=8$, $id=0.97$     & $\sim$ 2,750   & 475,465  \\
  \end{tabular}
  \caption{Performance of different clustering methods and different $k$-mer
  sizes. Sequence data:
           \texttt{RDP\_Pro\_Full\_sort.fna}. Count: 500,000. Throughput
           specifies the number of sequences clustered per second (including
           results output to file), but excludes reading the input file.}
\end{figure}
