\subsection{Distance metrics}

The following sections describe a range of different types of distance metrics,
that is, methods for measuring the distance between sequences.

\subsubsection{Edit distance}\label{sec:edit_distance}

One type of \emph{edit distance} is the \emph{Levenshtein
distance}~\cite{levenshtein}, which is a string metric for determining the
distance between two sequences. It is defined to be the minimum number of
edits to transform the first sequence into the other~\cite[p.~52]{dong}.

The edit operations consists of \emph{insertions}, \emph{deletions} and
\emph{substitutions}. These operations are, respectively, inserting a
character, removing a character and changing one character into another.

For example, the two sequences
\begin{center}
  \texttt{ACGT} \\
  \texttt{ACGGC}
\end{center}
would have a distance of \num{2} (e.g. substituting T for G and inserting a C).
However, there are some cases where the relevance of the distance is arguable.
Consider the sequences
\begin{center}
  \texttt{AATCATA\textcolor{blue}{CCGG}} \\
  \texttt{\textcolor{blue}{CCGG}AATCATA}
\end{center}
with a distance of \num{8}. This could be seen as a transposition of the
substring \texttt{CCGG}, which one might want to result in a low distance.

A bottom-up dynamic programming (DP) version of the Levenshtein distance metric
was implemented and tested on real DNA/RNA data to evaluate its performance. It
was clear that this algorithm is not suitable for large datasets, because it
would be infeasible to complete even a linear time clustering, in the number of
sequences to be clustered. The implementation could most likely be optimized
further, but because of the characteristics of an edit distance algorithm like
Levenshtein and because of the performance requirements for this project, it
was decided to not pursue further optimization and instead focus on other types
of algorithms. Furthermore, the Levenshtein distance penalizes transpositions,
i.e. it results in an increased distance.

The Levenshtein distance can still be used for evaluation and benchmarking
against other metrics. The algorithm is attached in appendix
\ref{app:levenshtein_algorithm} and results from tests are shown in section
\ref{sec:kdist_vs_levenshtein}.

A. Andoni and R. Krauthgamer~\cite[pp.~1--2]{andoni} state:
\begin{quote}
  ``The worst-case running time known for this problem has not improved in
  three decades (..)''.
\end{quote}
They further support the conclusion that the running time of the Levenshtein
algorithm is too poor for the large datasets of the problem domain.
Specifically the running time of Levenshtein is $\mathcal{O}(nm)$, where $n$
and $m$ are the lengths of the two sequences and $n\geq m$, and $\mathcal{O}(n
\cdot \max{(1,m/\log_2(n))})$ for a constant size alphabet~\cite{masek}.
The lack of improvements in performance for edit distance is one motivation
for the introduction of sequence alignment.


\subsubsection{Sequence alignment}

\emph{Sequence alignment} is used in bioinformatics to identify regions of
similarity by aligning the characters of two or more sequences in a certain
manner. Besides shifting a sequence to a side, gaps can be inserted between
characters as well. A gap is represented with `-' and it indicates an insertion
in one sequence or a deletion from another sequence. This is called an
\emph{indel}, which is a contraction of the first letters from insertion and
deletion. If it is not an indel and all characters in the column are the same,
then it is a match. Otherwise, it is a mismatch~\cite[pp.~135--136]{dong}.

Figure \ref{fig:seq_alignment} displays an alignment of two sequences. In
column 0 is an indel, in columns 1--3 are matches, in column
4 is a mismatch and so forth. The vertical bars in the middle
line indicate matches.

\begin{figure}[H]
  \centering
  \verb+ATGCAACGA+ \\
  \verb+ |||  |||+ \\
  \verb+-TGCG-CGA+
  \caption{Sequence alignment of ``ATGCAACGA'' and ``TGCGCGA''.}
  \label{fig:seq_alignment}
\end{figure}

Indels, gaps and mismatches can be given different weights in the calculation
of the similarity between the sequences.

Both \texttt{UCLUST} and the very recent project
\texttt{VSEARCH}~\cite{vsearch}, which is unpublished at the time of writing,
uses some kind of sequence alignment for comparing sequences. \texttt{VSEARCH}
uses a parallelized version of the dynamic programming algorithm
Needleman-Wunsch~\cite{needleman}, while \texttt{UCLUST} uses a heuristic
procedure by default, but can be instructed to use full dynamic programming
with either the Needleman-Wunsch or the Smith-Waterman~\cite{smith} algorithm.
However, this will make \texttt{UCLUST} much slower. The heuristic algorithm
used in \texttt{UCLUST} gives an approximation of the optimal alignment for the
purpose of increased performance.

As opposed to \texttt{USEARCH}, \texttt{VSEARCH} is free and open-source
software and is designed for 64-bit processors, so it does not have the
limitation on usable memory that the free, 32-bit version of \texttt{USEARCH}
has.

% Martin: Can we say something about VSEARCH performance?

Sequence alignment is still a computationally expensive operation and for
comparison of sequences with low similarity, the results of sequence alignment
are often poor. Additionally, as with the Levenshtein distance metric,
sequence alignment is very sensitive to the ordering of parts of sequences.
\emph{Alignment-free} sequence comparison methods~\cite{vinga} strive to
provide a measure of sequence similarity while avoiding the costly computation
that alignment involves. Alignment-free comparison allows one to look at each
of the sequences independently, extracting some characteristics, or
\emph{features}, of each sequence and then comparing these characteristics.
This can possibly reduce the complexity of the comparison to linear time or
better. One such alignment-free method is $k$-mer counting which will be
presented and analyzed in the following section.


\subsubsection{Feature based distance} \label{sec:kmer_distance}

A \emph{$k$-mer}, \emph{$k$-gram} or simply a \emph{word}, is a substring of
length $k>0$ over some alphabet $\mathcal{A}$ of a sequence. An interesting
feature of a sequence is which $k$-mers occur in that sequence and how many
times they occur. This is called $k$-mer counting and is a type of
\emph{feature based distance metric}.

The \emph{$d2$ distance metric} is a feature based distance metric, using
$k$-mers as the feature. The distance is calculated by counting the $k$-mers
occurring in two sequences, representing these occurrences as two vectors and
finally taking the Euclidean distance between these two
vectors~\cite[pp.~53--54]{dong}.

Let $c_x(w)$ be the number of times that a $k$-mer $w$ occurs in the sequence
$x$. Then the $d2$ distance between two sequences, $x$ and $y$, can be defined
as follows~\cite[pp.~1--2]{hazelhurst}
\[
  d2_k(x,y) \eqdef \sqrt{\sum_{w \in K(x) \cup K(y)} (c_x(w) - c_y(w))^2}
\]
where $K(x)$ and $K(y)$ denotes the set of $k$-mers in $x$ and $y$,
respectively.

As an example, the two $2$-mer frequency vectors of the sequences
\begin{align*}
  S_1 &= ACTACAC \\
  S_2 &= ACAGAT
\end{align*}
over the alphabet $\mathcal{A} = \{A,C,T,G\}$, can be illustrated as follows:

\begin{table}[!h]
\centering
\scalebox{0.75}{
\begin{tabular}{c | c c c c c c c c c c c c c c c c}
        & AA & AC & AG & AT & CA & CC & CG & CT & GA & GC & GG & GT & TA & TC & TG & TT \\
  \hline
  $S_1$ &    & 3  &    &    & 1  &    &    & 1  &    &    &    &    & 1  &    &    &    \\
  \hline
  $S_2$ &    & 1  & 1  & 1  & 1  &    &    &    & 1  &    &    &    &    &    &    &    \\
\end{tabular}}
\end{table}

The Euclidean distance would then be calculated as
\begin{align*}
  d2_2(S_1, S_2)
    &= \sqrt{(3-1)^2 + (-1)^2 + (-1)^2 + (1-1)^2 + 1^2 + (-1)^2 + 1^2} \\
    &= \sqrt{9} = 3
\end{align*}

The first version of the $d2$ distance metric algorithm described above is
shown in Algorithm \ref{alg:d2_basic} and has been implemented as well. The
$substring(i,k)$ method extracts $k$ characters from a string beginning
at position $i$. This algorithm maintains a single frequency vector, as a map
structure to allow for large $k$ which results in a large number of possible
different $k$-mers. When iterating through the first sequence, $k$-mer counts
are incremented and when iterating through the second sequence, they are
decremented. Finally all the frequencies are squared and the square root of
the sum is returned, corresponding to the Euclidean distance between frequency
vectors for the two sequences.

\begin{algorithm}[H]
  \caption{Basic \textsc{d2} distance metric}
  \label{alg:d2_basic}
  \begin{algorithmic}[1]
    \Require{$s$ and $t$ are DNA or RNA sequences and $k \in \mathbb{Z}^+$}
    \Statex
    \Function{d2}{$s, t, k$}
      \State initialize \texttt{freq\_map} of type \texttt{string $\to$ int} map
      \For{$i \gets 0$ to $\abs{s} - k$}
        \State \texttt{freq\_map}[$s.substring(i, k)$]\texttt{++}
      \EndFor
      \For{$i \gets 0$ to $\abs{t} - k$}
        \State \texttt{freq\_map}[$t.substring(i, k)$]\texttt{-{}-}
      \EndFor
      \State $total \gets 0$
      \ForAll{$e \in$ \texttt{freq\_map}}
        \State $total \gets total + e.value^2$
          \Comment{calculate the Euclidean distance}
      \EndFor
      \State \Return $\sqrt{total}$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

To better support the measurement of distance between two sequences of
different length, but similar substrings, the concept of a \emph{window} can
be introduced. In this context, a window is a conceptual construction which
gives a view of fixed length substrings of two sequences. This window can then
be moved character by character over the two sequences, calculating the
distance for each position of the window and then using the lowest of those
distances as the resulting distance. This concept is illustrated in Figure
\ref{fig:d2_window_concept} where the blue box corresponds to the window,
which in this case has the length 10.

\begin{figure}[H]
\centering
\begin{tikzpicture}
  \draw [fill=blue!25,dashed] (0,0.7) rectangle (2.95,1.8);
  \node at (2.1,1.5) {\dots AUGGAGAGUAUUGGCGACUC \dots};
  \node at (2.1,1.0) {\dots GGUCGCGAUGGUGCAUGGCAACCGGU \dots};
\end{tikzpicture}
\caption{Illustration of the concept of a window.}
\label{fig:d2_window_concept}
\end{figure}

For simplicity, the window size can be set equal to the length of the shortest
of the two sequences. In this way two sequences, where one is a substring of
the other, will have zero distance. Then the window only moves step by step
over the longer of the two sequences, while maintaining the position and the
$k$-mer counts for the shorter sequence.

Figure \ref{fig:d2_window_example} shows an example of this, where the $d2$
distance with the window is 0, since the first sequence is a prefix of the
other, while the $d2$ distance without a window gives a distance of 2.

\begin{figure}[H]
\centering
\begin{tikzpicture}
  \draw [fill=blue!10,dashed] (-0.45,0.5) rectangle (2.5,2.4);
  \draw [fill=blue!25,dashed] (-0.35,0.7) rectangle (1.15,2.2);
  \node at (1.0,1.5) {AGUAC\phantom{GGAU}};
  \node at (1.0,1.0) {AGUACGGAU};

  \node at (0.35, 2.0) {\footnotesize $d2_2 = 0$};
  \node at (1.8, 2.0) {\footnotesize $d2_2 = 2$};
\end{tikzpicture}
\caption{Example of difference in distance with window. The distance with the
  window is 0, since the first sequence is a prefix of the other, while the
  distance without the window is 2. This is for $k=2$.}
\label{fig:d2_window_example}
\end{figure}
